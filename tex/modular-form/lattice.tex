\chapter{Lattice over $\CC$}

% I recommend \cite{ref:koblitz} for a good introduction to modular form ---
% you don't need too much background knowledge, and it has a good style of writing.
% Here are some quotes:
% \begin{quote}
% No normal person likes to think in terms of ``equivalence classes'', [\dots]
% \end{quote}
% and (makes the reader at ease looking at a unnatural definition):
% \begin{quote}
% At first glance, it might seem simpler to define $Z(T)$ as $\sum N_r T^r$; however, [\dots]
% \end{quote}

Fundamentally, a modular form is something very simple:

\begin{moral}
	A modular form is a meromorphic homogeneous function on the space of lattices of $\CC$.
\end{moral}

% As simple as a modular form is, it sees a lot of uses everywhere, including the famous Fermat's
% last theorem.\todo{explain how}

But if you browse Wikipedia for example, you will see something like:
\begin{definition}[Confusing definition of a modular form]
	Let $H$ be the upper half-plane of $\CC$.

	Given a subgroup $\Gamma \subseteq \SL_2(\ZZ)$ such that the index $|\SL_2(\ZZ)/\Gamma|$
	is finite, a modular function of weight $k$ is a holomorphic function $f\colon H \to \CC$
	that satisfies:
	\begin{itemize}
		\ii automorphy condition: $f(\gamma(z)) = (cz+d)^k f(z) \forall \gamma \in \Gamma$,
		\ii growth condition: $(cz+d)^{-k} f(\gamma(z))$ is bounded $\forall \gamma \in \SL_2(\ZZ)$.
	\end{itemize}
\end{definition}

We will see what the intuition means, and why is it that the normal definition is so confusing.

\begin{remark}[Warning on notation]
The difference between modular form and modular function is usually that
a modular form is required to be ``holomorphic''
and a modular function only need to be ``meromorphic'',
but the notation is not consistent between different sources.
\end{remark}

\section{Lattice}

Everyone knows what a lattice is:

\begin{center}
	\begin{asy}
		import graph;
		real bound=6, extrax=2, extray=1;
		graph.xaxis("$\Re z$",-bound-extrax, bound+extrax);
		graph.yaxis("$\Im z$",-bound-extray, bound+extray, autorotate=false);
		for(int a=-10; a<=10; ++a){
			for(int b=-10; b<=10; ++b){
				pair p=(1, 0)*a+(0.4, 0.7)*b;
				if(abs(p.x)<=bound && abs(p.y)<=bound){
					dot(p, blue);
				}
			}
		}
	\end{asy}
\end{center}

Formally:
\begin{definition}
	A \vocab{lattice} of $\CC$ is a discrete subgroup of $\CC$
	that is isomorphic as an abelian group to $\ZZ^{\oplus 2}$.
\end{definition}

\begin{example}[Examples and non-examples of lattices of $\CC$]
	Here are some examples.
	\begin{itemize}
		\ii The set of Gaussian integers, $\ZZ[i]$, is a lattice.
		\ii If you have read the algebraic number theory chapter:
		For every quadratic imaginary number field $K$, the ring of integers $\OO_K$ is a lattice in
		$\CC$.
		\ii So is every ideal $\ka \subseteq \OO_K$, or every fractional ideal $J$.
	\end{itemize}
	But:
	\begin{itemize}
		\ii $\ZZ[\sqrt 2]$ is not a lattice.
		(It is not \emph{discrete} with respect to the topology of $\CC$.)
	\end{itemize}
\end{example}

Because a lattice is isomorphic to $\ZZ^{\oplus 2}$, we can specify a lattice with a basis
$(\omega_1, \omega_2)$. So the lattice can be written $\omega_1 \ZZ \oplus \omega_2 \ZZ$.

\begin{remark}
	This pair $(\omega_1, \omega_2)$ is called a \vocab{fundamental pair of periods} of the lattice.
\end{remark}

However, the basis is not unique.
\begin{example}[Examples of two different bases giving the same lattice]
	Consider $\ZZ[i]$. It is generated by $\langle 1, i \rangle$ as an abelian group.

	However, $\langle i, 1 \rangle$ is another basis. So is $\langle 1, -i \rangle$,
	or $\langle 1, 1+i \rangle$, or even $\langle 2+3i, 1+2i \rangle$.
\end{example}

Furthermore, consider the lattice $\langle 1, i \rangle$, $\langle 2, 2i \rangle$
and $\langle 1+i, 1-i \rangle$ ---
fundamentally they ``looks the same'' (they're identical up to rescaling), but it is not easy to
verify.

A question arises:
\begin{quote}
	How do we check if two lattices looks the same?
\end{quote}
While you could try a few methods such as computing the shortest vector in the lattice,
they're not \emph{analytic}.
Let us see what we can do with analytic methods.

\section{The Eisenstein series}

In this section, we wish to compute some sort of ``signature'' for the lattice.

\subsection{Definition}

We keep the same notation --- let $\omega_1, \omega_2 \in \CC$ be complex numbers
such that they generates a lattice.

Let $\Lambda = \{ a \omega_1+b \omega_2 \mid a, b \in \ZZ \}$ be the lattice.

In order for the signature to only depend on the lattice and not on the basis, ideally we want:
\begin{moral}
	The signature should be computed intrinsically from the lattice, and not the basis.
\end{moral}
And we also want the signature to be meromorphic --- no piecewise function allowed.

What can we do?

Let us try the following:
\[ \sum_{z \in \Lambda} z \]
Oops --- this doesn't converge. Let's try again:
\[ \sum_{z \in \Lambda} \frac{1}{z^4} \]
Well --- almost. As $|z|$ gets large, $|\frac{1}{z^4}|$ gets small fast enough for the series to
converge, but there's a $\frac{1}{0}$ there\dots

The real definition is the following:
\begin{definition}[Eisenstein series]
	For a lattice $\Lambda$ generated by $\omega_1$ and $\omega_2$, we define
	\[ G_k(\omega_1, \omega_2) = G_k(\Lambda) =
	\sum_{z \in \Lambda \setminus \{ 0 \}} \frac{1}{z^k}. \]
\end{definition}

\begin{example}
	Here are some special values of the function.
	\begin{itemize}
		\ii For $k \leq 2$, the series unfortunately doesn't converge.
		\ii $G_4(1, i) \approx 3.1512$.
		Closed form can be found in \url{https://mathoverflow.net/q/340154}.
		\ii $G_3(1, i) = 0$.
		\ii $G_3(1, \pi+e \cdot i) = 0$.
	\end{itemize}
\end{example}

You can write a quick program to numerically verify all of the above.

\begin{claim}
	For $k \geq 3$, $G_k$ is holomorphic in each variable $\omega_1$ and $\omega_2$.
\end{claim}
\begin{proof}[Sketch of Proof]
	Roughly speaking, each term $\frac{1}{z^k}$ is $\frac{1}{(a \omega_1+b \omega_2)^k}$
	which is holomorphic in each variable,
	and an absolutely convergent infinite sum of holomorphic functions
	is again holomorphic under suitable (local uniform convergence) conditions.
\end{proof}

\subsection{Wait, it vanishes everywhere with $k=3$?}

As you might have guessed, turns out $G_3(\omega_1, \omega_2) = 0$ for all $\omega_1, \omega_2$.
\begin{exercise}
	Prove it.
\end{exercise}

If you did the proof correctly, for every odd integer $k$, $G_k$ vanishes everywhere.
Thus we normally let $k \geq 4$ be even, and write $G_k$.

\subsection{The symmetry}

$G_k$ is all nice and easy to understand, but there's a problem: it is a function on two complex
variables, thus four real variables. How can you visualize a plot over four dimensions?

Let's see how we can simplify it. To start with, $G_k$ contains a lot of redundant information:
if we scale the lattice by a factor of $\lambda$, the result is multiplied by $\lambda^{-k}$.
(Just like a ``homogeneous polynomial'' of degree $-k$. Except it is not polynomial.)

So, you can write
\[ G_k(\omega_1, \omega_2) = G_k(1, \omega_2/\omega_1) \cdot \omega_1^{-k}. \]
Here are some more definitions for convenience.
\begin{definition}
	Define the \vocab{period ratio} $\tau$ to be the ratio between the two fundamental periods:
	\[ \tau = \frac{\omega_2}{\omega_1}. \]
\end{definition}
And write $G_k(\tau) = G_k(1, \tau)$ for short.

Of course we still have:
\begin{claim}
	$G_k(\tau)$ is holomorphic in $\tau$.
\end{claim}

More importantly, now it is a function of \emph{one} complex variable defined on $\CC \setminus \RR$
(note that if $\tau \in \RR$ then $(\omega_1, \omega_2)$ are collinear --- they doesn't form a
lattice) --- so we can visualize it by, for example, contour plot.

See \href{https://commons.wikimedia.org/wiki/File:KleinInvariantJ.jpg}{Wikipedia} for a plot of the
$j$-invariant, a modular form, in $\tau$.

Needless to say, there are another obvious thing we can eliminate:
the removal of $\RR$ from $\CC$ cuts it into two disconnected halves, and $G_k(\tau) =
G_k(-\tau)$. So we only need to consider a holomorphic function
on the \vocab{upper half-plane} $H$.

\begin{definition}
	Such a function $G_k\colon H \to \CC$ is called a \vocab{modular form} of \vocab{weight} $k$
	under suitable conditions.
\end{definition}

Okay, that is not quite true --- what about the growth condition?

\subsection{More symmetries}

If you look at the plot from Wikipedia\todo{add a self-contained plot} above, one thing is obvious:
the function keeps repeating left and right.
Formally, $G_k(\tau) = G_k(\tau+1)$ for all $\tau \in H$.

(You can easily see why:
the lattice $\langle 1, \tau \rangle$ is the same as the lattice $\langle 1, \tau+1 \rangle$.)

When is the last time we see a periodic function like that? Let's see\dots
\begin{itemize}
	\ii $\sin(x)$ and $\cos(x)$ with period $2 \pi$,
	\ii $e^x$ with period $2 \pi i$.
\end{itemize}
As you know, the first two cases are just derived from $e^x$.

To remove the redundancy, what can we do? One obvious thing to try is to substitute
\[ q = e^{2 \pi i \tau}. \]
Equivalently, write the function $G_k(\frac{\log q}{2 \pi i})$.

\missingfigure{plot}

\begin{claim}
	$q \mapsto G_k(\frac{\log q}{2 \pi i})$ is well-defined and holomorphic for $0<|q|<1$.
\end{claim}
While $\log q$ is only defined up to multiples of $2 \pi i$, the periodicity of $G_k$ cancels it
out.

\begin{remark}
	You can also substitute $q = e^{-2 \pi i \tau}$, then the function is defined for $1<|q|$ ---
	but the other way is certainly easier to wrap your head (or the unit circle) around.
\end{remark}

While doing this, we notice something:
\begin{moral}
	The function is also holomorphic at $q=0$!
\end{moral}

It is easy to check that as $\Im \tau \to \infty$, then $G_k(\tau)$ converges to a fixed number,
thus the function in terms of $q$ is bounded in a small neighborhood of $q=0$.
By Riemann's removable singularity theorem,
we can extend the function to be holomorphic at $q=0$ as well.

The point $q=0$ is called a \vocab{cusp}. (The reason why will be explained later.) \todo{explain}
\begin{moral}
	You should think of it as the ``point at infinity'' in terms of $\tau$.
\end{moral}

\begin{exercise}
	Find out \emph{which} fixed number it converges to exactly. (Answer below.)
\end{exercise}

The answer is simple: over the lattice points $a+b \tau$, if $\Im \tau \to \infty$, then the points
with $b \neq 0$ will have $|a+b \tau| \to \infty$, thus $\frac{1}{(a+b \tau)^k}\to 0$.
With some work (uniform convergence, etc.) you can formalize it.

For notational simplicity, we define:
\begin{definition}
	Let $\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s}$.
\end{definition}
Then $G_k(\tau) \to 2 \zeta(k)$ as $\Im \tau \to \infty$.

\begin{remark}
	This innocuous-looking function is the \vocab{Riemann zeta function}, the main subject of the
	infamous Riemann hypothesis.
\end{remark}

\subsection{Relation to Fourier transform}

In the previous section, you have accidentally perform a Fourier transform of $G_k(\tau)$!

``\emph{I thought the Fourier transform is more complicated!}'' you may say.
Fortunately, it is actually that simple.

Consider the horizontal line $\Im \tau = \frac{1}{10}$.

This corresponds to the circle $|q| = e^{-2 \pi/10} \approx 0.533$.

\missingfigure{plot}

If a holomorphic function has power series expansion at $q=0$ to be
\[ a_0+a_1 q+a_2 q^2+a_3 q^3+\cdots \]
let us see how each summand would look like on the circle $|q|\approx 0.533$.
\begin{itemize}
	\ii $a_0$ would just be a constant, adding $a_0$ everywhere.
	\ii $a_1 q$ would rotate once around origin as $q$ traverse once around the circle.

	Algebraically:
	\[ a_1 q = a_1 \cdot e^{2 \pi i \tau}
	= a_1 e^{-2 \pi \Im\tau} \cdot (\cos (2i \Re \tau) + i \sin (2i \Re \tau)).  \]
	\ii Similarly, $a_2 q^2$ would rotate twice around origin as $q$ traverse once around the
	circle.
	\[ a_2 q^2 = a_1 \cdot e^{2 \pi i \cdot 2 \tau}
	= a_1 e^{-4 \pi \Im\tau} \cdot (\cos (4i \Re \tau) + i \sin (4i \Re \tau)).  \]
\end{itemize}
See, each $a_i$ is in fact the \emph{amplitude} of each frequency along the line!
You can derive the normal formula of Fourier transform with a contour integral.

\subsection{The $q$-expansion}

Unsurprisingly, the Taylor series expansion around $q=0$ is called the \vocab{$q$-expansion} of the
modular form.

Turns out, some of the values are the following. Assume we're looking at $k=4$, that is $G_k(q) =
G_4(q)$:
\begin{itemize}
	\ii $a_0 = 2 \zeta(4)$, as we already computed above.
	\ii $a_1 = 480 \zeta(4)$.
	\ii $a_1 = 4320 \zeta(4)$.
	\ii $a_1 = 13440 \zeta(4)$.
\end{itemize}

``They're all integers?'' Another surprise! Given how $G_4(i)$ isn't even rational.

\todo{explain why}

\begin{remark}
	That said, some ratios such as $G_4(\frac{i}{2})/G_4(i)=11$ are rational ---
	there is even an elementary proof of this fact.
	Most aren't, however.
\end{remark}

\section{Modular form}

\subsection{Definition}
\prototype{The Eisenstein series $G_k$.}

Now, the official definition:
\begin{definition}
	A \vocab{modular form} of \vocab{weight} $k$ is a holomorphic function $f(q)$ as above,
	such that if we define $f(\omega_1, \omega_2)$ by the procedure above, then:
	\begin{itemize}
		\ii $f$ evaluates to the \emph{same value} on the same lattice,
		\ii $f(\lambda \omega_1, \lambda \omega_2) = \lambda^{-k} f(\omega_1, \omega_2)$
		for all nonzero $\lambda \in \CC$ and non-collinear $\omega_1$ and $\omega_2$.
	\end{itemize}

	A \vocab{modular function} of \vocab{weight} $k$ is a meromorphic function $f(q)$ satisfying the
	same hypotheses.
\end{definition}

\begin{example}[Example of modular forms and modular functions]
	\listhack
	\begin{itemize}
		\ii $G_4$ is a modular form of weight $4$.
		\ii $G_6$ is a modular form of weight $6$.
		\ii $\frac{G_8}{G_4^2}$ is a modular \emph{function} of weight $0$ ---
		that is, it remains the same when the lattice is rescaled!
		\ii So is $\frac{G_6^2}{G_4^3}$.
	\end{itemize}
\end{example}

\begin{exercise}
	Prove that if the exponent is $3$ (so the ``weight'' is $1.5$),
	then a function $f$ satisfying the hypotheses above must be identically $0$.
\end{exercise}

Let us translate to $f(\tau)$ notation.
The second condition, $f(\lambda \omega_1, \lambda \omega_2) = \lambda^{-k} f(\omega_1, \omega_2)$,
turns out to be tautological.

What about the first condition?
We obviously need:
\begin{itemize}
	\ii $f(\omega_1, \omega_2) = f(\omega_1+\omega_2, \omega_2)$,
	\ii $f(\omega_1, \omega_2) = f(\omega_2, \omega_1)$,
	\ii $f(\omega_1, \omega_2) = f(-\omega_1, \omega_2)$,
	\ii etc.
\end{itemize}

How do we know that we're exhaustive? Well, notice the following:
\begin{exercise}
	Suppose $\langle \omega_1, \omega_2 \rangle$ and $\langle \alpha_1, \alpha_2 \rangle$
	generate the same lattice. Show that $\alpha_2 \in \langle \omega_1, \omega_2 \rangle$.
\end{exercise}
Thus $\alpha_2 = a  \cdot \omega_2+b  \cdot \omega_1$ for some $a, b \in \ZZ$.
Similarly, we write:
\begin{itemize}
	\ii $\alpha_1 = c  \cdot \omega_2+d  \cdot \omega_1$ for some $c, d \in \ZZ$,
	\ii $\omega_2 = a' \cdot \alpha_2+b' \cdot \alpha_1$ for some $a', b' \in \ZZ$.
	\ii $\omega_1 = c' \cdot \alpha_2+d' \cdot \alpha_1$ for some $c', d' \in \ZZ$,
\end{itemize}

Written in formula, we have
\[ \begin{pmatrix} a&b \\ c&d \end{pmatrix} \begin{pmatrix} \omega_2 \\ \omega_1 \end{pmatrix}
= \begin{pmatrix} \alpha_2 \\ \alpha_1 \end{pmatrix} \]
and
\[ \begin{pmatrix} a'&b' \\ c'&d' \end{pmatrix} \begin{pmatrix} \alpha_2 \\ \alpha_1 \end{pmatrix}
= \begin{pmatrix} \omega_2 \\ \omega_1 \end{pmatrix}. \]

\begin{exercise}
	Prove that if $a, b, c, d, a', b', c', d' \in \ZZ$ as above exists, then the converse holds:
	$\langle \omega_1, \omega_2 \rangle$ and $\langle \alpha_1, \alpha_2 \rangle$
	generate the same lattice.
\end{exercise}

So the two matrices are in $\GL_2(\ZZ)$, and are \emph{inverses} of each other.
Thus their \emph{determinant} must be invertible in $\ZZ$, so must be $\pm 1$.

Translated to $\tau$ notation, we have (where $\tau = \frac{\omega_2}{\omega_1}$):
\begin{align*}
	f(\omega_1, \omega_2) &= f(\alpha_1, \alpha_2) \\
	\iff f\left(\frac{\omega_2}{\omega_1}\right) \omega_1^{-k}
						  &= f\left(\frac{\alpha_2}{\alpha_1}\right) \alpha_1^{-k} \\
	\iff (c \tau+d)^k f(\tau) &= f\left(\frac{a \tau+b}{c \tau+d}\right).
\end{align*}

\begin{exercise}
	Assume both $\tau$ and $\frac{a \tau+b}{c \tau+d}$ are in the upper half-plane $H$.
	Prove that the determinant $ad-bc$ cannot be $-1$.
\end{exercise}

So the determinant is $1$, equivalently,
$\begin{pmatrix} a&b \\ c&d \end{pmatrix} \in \SL_2(\ZZ)$.

That explains the commonly-seen textbook definition.

For more notational convenience, we define:
\begin{definition}
	For $g = \begin{pmatrix} a&b \\ c&d \end{pmatrix} \in \SL_2(\ZZ)$,
	define $g \tau = \frac{a \tau+b}{c \tau+d}$.
\end{definition}

\subsection{Relation to Riemann surfaces}

Consider a modular function of weight $0$, such as $\frac{G_6^2}{G_4^3}$.
(There is actually no non-constant modular \emph{form} of weight $0$.)

If you have read the Riemann surfaces chapter, you can immediately see the relation here:
\begin{moral}
	A modular function of weight $0$ is a meromorphic function on the Riemann surface formed by
	taking $H$, fusing $\tau$ and $g \tau$ together for all $g \in \SL_2(\ZZ)$,
	then put a neighborhood of $q=0$ on top of it to close it up.
\end{moral}

See --- it is \emph{natural} to stick pieces of $\CC$ together!

\begin{theorem}
	The Riemann surface formed as above is compact.
\end{theorem}

Just like $\CP^1$, the Riemann sphere.\todo{explain significance}

